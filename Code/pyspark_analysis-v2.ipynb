{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef6867c-bf48-4934-9102-ab229e27b90d",
   "metadata": {},
   "source": [
    "# Data Analysis and PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd108466-2e29-4f24-898a-3ee8a2832af2",
   "metadata": {},
   "source": [
    "## Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19dc74a7-f334-4d73-aacd-aea3d21cccd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/home/subha/miniconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/subha/miniconda3/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b1162-657c-4d51-9706-3e89e3c917a7",
   "metadata": {},
   "source": [
    "## Starting Pyspark with Master 10G Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc8bb66-c94a-447b-93bf-7403e8abaea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 14:20:38 WARN Utils: Your hostname, neoshiva resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/02/23 14:20:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/23 14:20:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/subha/aiwork/spark')\n",
    "# Initializing the spark context\n",
    "#import pyspark.pandas as ps\n",
    "#pdf_incidents = df_incidents.to_pandas_on_spark()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Configure Spark to use multiple threads\n",
    "spark = SparkSession.builder.appName(\"Amazon Reviews Analysis\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"/home/subha/miniconda3/bin/python\")\\\n",
    "    .config(\"spark.driver.maxResultSize\",\"10g\")\\\n",
    "    .config(\"spark.executor.instances\", \"4\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8be2d7-5452-4faa-b932-a533cd341c29",
   "metadata": {},
   "source": [
    "## Gathering Train and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca33b4f-be46-4990-a386-77ba14c596de",
   "metadata": {},
   "source": [
    "### Reading Test and Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deee1fd6-ca2b-4291-8803-b6f8a8948358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.text(\"amazon_train_dataset/test.ft.txt\")\n",
    "df_train = spark.read.text(\"amazon_train_dataset/train.ft.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19a37b-8e4b-436c-86dd-06014ce843c0",
   "metadata": {},
   "source": [
    "### Shaping the Datasets (Feature Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f27ce22-883e-4893-b0d2-aec9b8611afc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the first few rows to understand the data format\n",
    "def shaping_datasets(df):\n",
    "    leng = len(\"__label__1\")\n",
    "    # Split the text into two parts: label and review text\n",
    "    df_split = df.withColumn(\"label\", split(col(\"value\"), \" \").getItem(0))\\\n",
    "    .withColumn(\"review_text\", substring(df.value,12,10000))  # get review text from the second word onward)\n",
    "    \n",
    "    # Map label to sentiment: \"negative\" for __label__1, \"positive\" for __label__2\n",
    "    # df_sentiment_reviews = df_split.withColumn(\n",
    "    #     \"sentiment\",\n",
    "    #     when(col(\"label\") == \"__label__1\", \"negative\")\n",
    "    #     .when(col(\"label\") == \"__label__2\", \"positive\")\n",
    "    #     .otherwise(\"unknown\")\n",
    "    # )\n",
    "    df_sentiment_reviews = df_split.withColumn(\n",
    "        \"sentiment\",\n",
    "        when(col(\"label\") == \"__label__1\", 0)\n",
    "        .when(col(\"label\") == \"__label__2\", 1)\n",
    "        .otherwise(\"unknown\")\n",
    "    )\n",
    "    \n",
    "    # Select only the relevant columns: sentiment and review text\n",
    "    df_sentiment_reviews = df_sentiment_reviews.select(\"sentiment\", \"review_text\")\n",
    "    \n",
    "    return df_sentiment_reviews\n",
    "\n",
    "## Calling the Function\n",
    "df_test_p = shaping_datasets(df_test)\n",
    "df_train_p = shaping_datasets(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfae8fd-4c5f-4a14-a52f-e4684aa75d94",
   "metadata": {},
   "source": [
    "### Display and Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ab3f238-f236-44b6-91f8-032739b2b435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|sentiment|         review_text|\n",
      "+---------+--------------------+\n",
      "|        1|Stuning even for ...|\n",
      "|        1|The best soundtra...|\n",
      "|        1|Amazing!: This so...|\n",
      "|        1|Excellent Soundtr...|\n",
      "|        1|Remember, Pull Yo...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_train_p.show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440d8b9c-092f-4e1f-a231-299dffd43560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3600000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_p.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23bff12-675b-4ec2-b29c-f6ef09143891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|sentiment|         review_text|\n",
      "+---------+--------------------+\n",
      "|        1|Great CD: My love...|\n",
      "|        1|One of the best g...|\n",
      "|        0|Batteries died wi...|\n",
      "|        1|works fine, but M...|\n",
      "|        1|Great for the non...|\n",
      "|        0|DVD Player crappe...|\n",
      "|        0|Incorrect Disc: I...|\n",
      "|        0|DVD menu select p...|\n",
      "|        1|Unique Weird Orie...|\n",
      "|        0|Not an \"ultimate ...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test_p.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f54378d-5f8b-4b7a-995f-7a0e6fc94307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_p.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ce633-4241-42dd-a8f0-1f73017f5b1b",
   "metadata": {},
   "source": [
    "## Natural Language Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37e0ce-12a9-413f-a18d-4f35461b8034",
   "metadata": {},
   "source": [
    "### NLTK resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bd9e990-0bed-4de3-87bc-22a28f3ad1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/subha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/subha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/subha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/subha/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993352b-06cd-47e6-a8aa-b4b361ab0f84",
   "metadata": {},
   "source": [
    "### Lemmetizer and Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9e18111-c5d6-4da6-a3f1-344b48040bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b898f-9505-47f3-932a-73b271c0ac1c",
   "metadata": {},
   "source": [
    "### Clean, Tokenize and UDF creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0427fd5-2b02-42f5-8b54-f3cf121e3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing special characters, numbers, and converting to lowercase\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_and_preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize, remove stopwords, and lemmatize text\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "    #          if token.lower() not in stop_words and len(token) > 2]\n",
    "    tokens = [token for token in tokens \n",
    "             if token.lower() not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "# Register UDFs\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "tokenize_and_preprocess_udf = udf(tokenize_and_preprocess, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a4cbd-202a-4f77-a493-8ac3ee0774e8",
   "metadata": {},
   "source": [
    "### NLTK Review Process Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c686eefd-9565-4d2c-a55f-dbbcc035def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_with_nltk(reviews_df):\n",
    "    \"\"\"\n",
    "    Process reviews using NLTK for text cleaning and preprocessing\n",
    "    \n",
    "    Args:\n",
    "        reviews_df: DataFrame with 'sentiment' and 'reviews' columns\n",
    "    Returns:\n",
    "        DataFrame with processed text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    processed_df = reviews_df.withColumn(\n",
    "        \"cleaned_text\",\n",
    "        clean_text_udf(col(\"review_text\"))\n",
    "    )\n",
    "    \n",
    "    # Apply tokenization, stopword removal, and lemmatization\n",
    "    processed_df = processed_df.withColumn(\n",
    "        \"processed_tokens\",\n",
    "        tokenize_and_preprocess_udf(col(\"cleaned_text\"))\n",
    "    )\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    processed_df = processed_df.withColumn(\n",
    "        \"processed_text\",\n",
    "        udf(lambda x: ' '.join(x) if x else '', StringType())(col(\"processed_tokens\"))\n",
    "    )\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ad9c5-2025-4620-8b98-3b1fc6248b2a",
   "metadata": {},
   "source": [
    "## Executing the Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e36523-afe6-450b-915a-4018ff7ea7da",
   "metadata": {},
   "source": [
    "### Test and Train Spark DF created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0e99b7b-fba2-4d38-95b4-c0cc5e013b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sentiment: string (nullable = false)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- processed_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- processed_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_processed_reviews_df = process_reviews_with_nltk(df_test_p)\n",
    "train_processed_reviews_df = process_reviews_with_nltk(df_train_p)\n",
    "test_processed_reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808d7af-2aef-4643-b26c-1f993fc9a2b1",
   "metadata": {},
   "source": [
    "### Writing to Output Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90a32be1-b36d-4d0c-84b6-ff3944619e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "test_processed_reviews_df.select(\"cleaned_text\",\"sentiment\").coalesce(1).write.mode(\"overwrite\").parquet(\"output/cleandata/test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60bb508e-ea84-452d-9b11-a4fbe5a0ce6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "train_processed_reviews_df.select(\"cleaned_text\",\"sentiment\").coalesce(1).write.mode(\"overwrite\").parquet(\"output/cleandata/train_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f608a-8699-484a-9080-0b7f6f1a2cbb",
   "metadata": {},
   "source": [
    "### Creating Samples for hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b883ab2-c8d0-470c-a31d-b2cb1468cd2a",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "02c5009f-a41f-4056-bc49-e73c16bdf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed_reviews_df.createOrReplaceTempView(\"train_processed_reviews_df\")\n",
    "test_processed_reviews_df.createOrReplaceTempView(\"test_processed_reviews_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcd01817-9c1f-4408-b460-f269634c95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select * \n",
    "from train_processed_reviews_df \n",
    "where sentiment = 0  limit 180000\n",
    "\"\"\"\n",
    "tr_df_1 = spark.sql(sql)\n",
    "\n",
    "sql =\"\"\"\n",
    "select * \n",
    "from train_processed_reviews_df \n",
    "where sentiment = 1  limit 180000\n",
    "\"\"\"\n",
    "tr_df_2 = spark.sql(sql)\n",
    "\n",
    "u_tr_df = tr_df_1.union(tr_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2b55ef3-0ea3-4699-b3bd-2df8066a3163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "u_tr_df.select(\"cleaned_text\",\"sentiment\").coalesce(1).write.mode(\"overwrite\").parquet(\"output/cleandata/train_data_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c3461-fe22-40fd-9ef4-28ccc2ea8724",
   "metadata": {},
   "source": [
    "#### Test and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92ab5021-baed-44be-ad04-80131d5b3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select * \n",
    "from test_processed_reviews_df \n",
    "where sentiment = 0  limit 20000\n",
    "\"\"\"\n",
    "te_df_1 = spark.sql(sql)\n",
    "val_df_1, test_df_1 = te_df_1.randomSplit([0.5, 0.5]) \n",
    "\n",
    "sql1 =\"\"\"\n",
    "select * \n",
    "from test_processed_reviews_df \n",
    "where sentiment = 1  limit 20000\n",
    "\"\"\"\n",
    "te_df_2 = spark.sql(sql1)\n",
    "val_df_2, test_df_2 = te_df_2.randomSplit([0.5, 0.5]) \n",
    "\n",
    "u_val_te_df = val_df_1.union(val_df_2)\n",
    "u_test_te_df = test_df_1.union(test_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9372075-b5ea-41b3-b285-60de43770388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:======================================================> (31 + 1) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10032 9913 9968 10087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(val_df_1.count(),val_df_2.count(),test_df_1.count(),test_df_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c03a3c0e-a5e8-4a45-8b96-530f84d7f8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "u_val_te_df.select(\"cleaned_text\",\"sentiment\").coalesce(1).write.mode(\"overwrite\").parquet(\"output/cleandata/val_data_sample\")\n",
    "u_test_te_df.select(\"cleaned_text\",\"sentiment\").coalesce(1).write.mode(\"overwrite\").parquet(\"output/cleandata/test_data_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc216946-6300-4683-b788-ee9a2cb1c721",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f9c6bd3-769b-4d75-ae9d-36142e719c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sentiment|         review_text|        cleaned_text|    processed_tokens|      processed_text|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        1|Great CD: My love...|great cd my lovel...|[great, lovely, p...|great lovely pat ...|\n",
      "|        1|One of the best g...|one of the best g...|[one, best, game,...|one best game mus...|\n",
      "|        0|Batteries died wi...|batteries died wi...|[batteries, died,...|batteries died wi...|\n",
      "|        1|works fine, but M...|works fine but ma...|[works, fine, mah...|works fine maha e...|\n",
      "|        1|Great for the non...|great for the non...|[great, nonaudiop...|great nonaudiophi...|\n",
      "|        0|DVD Player crappe...|dvd player crappe...|[dvd, player, cra...|dvd player crappe...|\n",
      "|        0|Incorrect Disc: I...|incorrect disc i ...|[incorrect, disc,...|incorrect disc lo...|\n",
      "|        0|DVD menu select p...|dvd menu select p...|[dvd, menu, selec...|dvd menu select p...|\n",
      "|        1|Unique Weird Orie...|unique weird orie...|[unique, weird, o...|unique weird orie...|\n",
      "|        0|Not an \"ultimate ...|not an ultimate g...|[ultimate, guide,...|ultimate guide fi...|\n",
      "|        1|Great book for tr...|great book for tr...|[great, book, tra...|great book travel...|\n",
      "|        0|Not!: If you want...|not if you want t...|[want, listen, du...|want listen duke ...|\n",
      "|        0|A complete Bust: ...|a complete bust t...|[complete, bust, ...|complete bust gam...|\n",
      "|        1|TRULY MADE A DIFF...|truly made a diff...|[truly, made, dif...|truly made differ...|\n",
      "|        0|didn't run off of...|didnt run off of ...|[didnt, run, usb,...|didnt run usb bus...|\n",
      "|        0|Don't buy!: First...|dont buy first of...|[dont, buy, first...|dont buy first co...|\n",
      "|        1|Simple, Durable, ...|simple durable fu...|[simple, durable,...|simple durable fu...|\n",
      "|        1|Review of Kelly C...|review of kelly c...|[review, kelly, c...|review kelly club...|\n",
      "|        1|SOY UN APASIONADO...|soy un apasionado...|[soy, apasionado,...|soy apasionado de...|\n",
      "|        1|Some of the best ...|some of the best ...|[best, fiddle, pl...|best fiddle playi...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_processed_reviews_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5082cc88-68ab-4ece-9e15-eaca7037888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_processed_reviews_df.createOrReplaceTempView(\"test_processed_reviews_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f97c97ca-9ba9-4daf-bd46-042a4ec50b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|count(1)|sentiment|\n",
      "+--------+---------+\n",
      "|  200000|        0|\n",
      "|  200000|        1|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"select count(1),sentiment from test_processed_reviews_df group by sentiment \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae0e215-45fc-40ca-b697-cdd6e3ece88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed_reviews_df.createOrReplaceTempView(\"train_processed_reviews_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94a7ef55-3717-405b-8c39-e1339a876d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=>                                                      (1 + 31) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|count(1)|sentiment|\n",
      "+--------+---------+\n",
      "| 1800000|        0|\n",
      "| 1800000|        1|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1),sentiment from train_processed_reviews_df group by sentiment \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584343cc-71e8-434c-aea4-bbd7da966c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select * \n",
    "from train_processed_reviews_df \n",
    "where sentiment = 0  limit 180000\n",
    "\"\"\"\n",
    "tr_df_1 = spark.sql(sql)\n",
    "\n",
    "\"\"\"\n",
    "select * \n",
    "from train_processed_reviews_df \n",
    "where sentiment = 1  limit 180000\n",
    "\"\"\"\n",
    "tr_df_2 = spark.sql(sql)\n",
    "\n",
    "u_tr_df = tr_df_1.union(tr_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec372d42-5916-4b66-9a5b-b0e18ca1a11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "360000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_tr_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fbe1f3c-9606-4f47-b80e-b45beb775b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:======================================================> (31 + 1) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sentiment|         review_text|        cleaned_text|    processed_tokens|      processed_text|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        0|Buyer beware: Thi...|buyer beware this...|[buyer, beware, s...|buyer beware self...|\n",
      "|        0|The Worst!: A com...|the worst a compl...|[worst, complete,...|worst complete wa...|\n",
      "|        0|Oh please: I gues...|oh please i guess...|[please, guess, r...|please guess roma...|\n",
      "|        0|Awful beyond beli...|awful beyond beli...|[awful, beyond, b...|awful beyond beli...|\n",
      "|        0|Don't try to fool...|dont try to fool ...|[dont, try, fool,...|dont try fool fak...|\n",
      "|        0|sizes recomended ...|sizes recomended ...|[sizes, recomende...|sizes recomended ...|\n",
      "|        0|mens ultrasheer: ...|mens ultrasheer t...|[mens, ultrasheer...|mens ultrasheer m...|\n",
      "|        0|Another Abysmal D...|another abysmal d...|[another, abysmal...|another abysmal d...|\n",
      "|        0|Problem with char...|problem with char...|[problem, chargin...|problem charging ...|\n",
      "|        0|Works, but not as...|works but not as ...|[works, advertise...|works advertised ...|\n",
      "|        0|Disappointed: I r...|disappointed i re...|[disappointed, re...|disappointed read...|\n",
      "|        0|Oh dear: I was ex...|oh dear i was exc...|[dear, excited, f...|dear excited find...|\n",
      "|        0|Incorrect disc!: ...|incorrect disc i ...|[incorrect, disc,...|incorrect disc bi...|\n",
      "|        0|should be titled ...|should be titled ...|[titled, hollywoo...|titled hollywood ...|\n",
      "|        0|Nothing you don't...|nothing you dont ...|[nothing, dont, a...|nothing dont alre...|\n",
      "|        0|Unfortunately it ...|unfortunately it ...|[unfortunately, w...|unfortunately was...|\n",
      "|        0|Doesn't work on a...|doesnt work on a ...|[doesnt, work, ma...|doesnt work mac c...|\n",
      "|        0|Very Frustrating:...|very frustrating ...|[frustrating, thr...|frustrating three...|\n",
      "|        0|Mind numbing: Thi...|mind numbing this...|[mind, numbing, g...|mind numbing game...|\n",
      "|        0|Cannot recommend:...|cannot recommend ...|[recommend, forme...|recommend former ...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "u_tr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "297c9059-3e29-457b-a619-01639dfeda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select * \n",
    "from test_processed_reviews_df \n",
    "where sentiment = 0  limit 20000\n",
    "\"\"\"\n",
    "te_df_1 = spark.sql(sql)\n",
    "val_df_1, test_df_1 = te_df_1.randomSplit([0.5, 0.5]) \n",
    "\n",
    "\"\"\"\n",
    "select * \n",
    "from test_processed_reviews_df \n",
    "where sentiment = 1  limit 20000\n",
    "\"\"\"\n",
    "te_df_2 = spark.sql(sql)\n",
    "val_df_2, test_df_2 = te_df_2.randomSplit([0.5, 0.5]) \n",
    "\n",
    "u_val_te_df = val_df_1.union(val_df_1)\n",
    "u_test_te_df = test_df_1.union(test_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cfe4ca8-2e81-4b3e-8964-58c49602fb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:======================================================> (31 + 1) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sentiment|         review_text|        cleaned_text|    processed_tokens|      processed_text|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        0|!!!Not enough to ...|not enough to get...|[enough, get, goo...|enough get good r...|\n",
      "|        0|!!!PLEASE NOTE!!!...|please note pleas...|[please, note, pl...|please note pleas...|\n",
      "|        0|\" Smile\"............|smileit never rea...|[smileit, never, ...|smileit never rea...|\n",
      "|        0|\" this is not it....|this is not it th...|[barto, reviewer,...|barto reviewer ac...|\n",
      "|        0|\"And the Band Pla...|and the band play...|[band, played, pu...|band played pulle...|\n",
      "|        0|\"Baby Dry\" but Co...|baby dry but cove...|[baby, dry, cover...|baby dry covered ...|\n",
      "|        0|\"Be Afraid, Be Ve...|be afraid be very...|[afraid, afraid, ...|afraid afraid def...|\n",
      "|        0|\"Beware! Take car...|beware take care ...|[beware, take, ca...|beware take care ...|\n",
      "|        0|\"But what are her...|but what are her ...|[chance, movie, g...|chance movie grea...|\n",
      "|        0|\"Close But No Cig...|close but no ciga...|[close, cigar, st...|close cigar stein...|\n",
      "|        0|\"Cue the Short Ga...|cue the short gan...|[cue, short, gang...|cue short gangste...|\n",
      "|        0|\"Culinary Holy Gr...|culinary holy gra...|[culinary, holy, ...|culinary holy gra...|\n",
      "|        0|\"Everyone\" stay a...|everyone stay awa...|[everyone, stay, ...|everyone stay awa...|\n",
      "|        0|\"Fact\" plus ficti...|fact plus fiction...|[fact, plus, fict...|fact plus fiction...|\n",
      "|        0|\"German Wirehaire...|german wirehaired...|[german, wirehair...|german wirehaired...|\n",
      "|        0|\"Hooray For Boobi...|hooray for boobie...|[hooray, booby, d...|hooray booby does...|\n",
      "|        0|\"I WAS FORCED TO ...|i was forced to r...|[forced, read, bo...|forced read book ...|\n",
      "|        0|\"I don't like Cra...|i dont like cradl...|[dont, like, crad...|dont like cradle ...|\n",
      "|        0|\"I think they los...|i think they lost...|[think, lost, bal...|think lost ball w...|\n",
      "|        0|\"I think\" and \"I ...|i think and i bel...|[think, beleive, ...|think beleive tha...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "u_val_te_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be6464cd-d9a6-4eb4-ba94-f1fd8472816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:=================================================>     (29 + 3) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|sentiment|         review_text|        cleaned_text|    processed_tokens|      processed_text|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "|        0|!!!! DO NOT PURCH...|do not purchase t...|[purchase, thermo...|purchase thermome...|\n",
      "|        0|\" Smile\"............|smileit never rea...|[smileit, never, ...|smileit never rea...|\n",
      "|        0|\" this is not it....|this is not it th...|[barto, reviewer,...|barto reviewer ac...|\n",
      "|        0|\"1\" star, because...|star because of t...|[star, book, good...|star book good fi...|\n",
      "|        0|\"And the Band Pla...|and the band play...|[band, played, pu...|band played pulle...|\n",
      "|        0|\"Baby Dry\" but Co...|baby dry but cove...|[baby, dry, cover...|baby dry covered ...|\n",
      "|        0|\"Be Afraid, Be Ve...|be afraid be very...|[afraid, afraid, ...|afraid afraid def...|\n",
      "|        0|\"Call me disapoin...|call me disapoint...|[call, disapointe...|call disapointed ...|\n",
      "|        0|\"Case Closed\"!?!?...|case closed in th...|[case, closed, au...|case closed autho...|\n",
      "|        0|\"Close But No Cig...|close but no ciga...|[close, cigar, st...|close cigar stein...|\n",
      "|        0|\"Cue the Short Ga...|cue the short gan...|[cue, short, gang...|cue short gangste...|\n",
      "|        0|\"Culinary Holy Gr...|culinary holy gra...|[culinary, holy, ...|culinary holy gra...|\n",
      "|        0|\"Fact\" plus ficti...|fact plus fiction...|[fact, plus, fict...|fact plus fiction...|\n",
      "|        0|\"German Wirehaire...|german wirehaired...|[german, wirehair...|german wirehaired...|\n",
      "|        0|\"House Atreides\" ...|house atreides is...|[house, atreides,...|house atreides du...|\n",
      "|        0|\"I LOVE YOU I LOV...|i love you i love...|[love, love, hate...|love love hate wo...|\n",
      "|        0|\"I love you, litt...|i love you little...|[love, little, on...|love little one o...|\n",
      "|        0|\"I think\" and \"I ...|i think and i bel...|[think, beleive, ...|think beleive tha...|\n",
      "|        0|\"Jonathan Livings...|jonathan livingst...|[jonathan, living...|jonathan livingst...|\n",
      "|        0|\"Kane Hodder shou...|kane hodder shoul...|[kane, hodder, pl...|kane hodder playe...|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "u_test_te_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45fbe5db-f16b-4d6a-8547-4b19232f6b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split1, split2 = te_df_1.randomSplit([0.5, 0.5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0354e-58af-4ce6-93e6-aa07df6f1c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select * \n",
    "from test_processed_reviews_df \n",
    "where sentiment = 0  limit 10000\n",
    "\"\"\"\n",
    "te_df_1 = spark.sql(sql)\n",
    "\n",
    "\"\"\"\n",
    "select * \n",
    "from test_processed_reviews_df \n",
    "where sentiment = 1  limit 10000\n",
    "\"\"\"\n",
    "te_df_2 = spark.sql(sql)\n",
    "\n",
    "u_te_df = te_df_1.union(te_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d606ecfd-5001-4e65-a3eb-7b53c112a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_val_te_df.createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20258aae-6c0a-42d6-9abf-7962cd52233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:====================================================>   (30 + 2) / 32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|count(1)|sentiment|\n",
      "+--------+---------+\n",
      "|    9976|        0|\n",
      "|   10110|        1|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(1),sentiment from test group by sentiment\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eecd82-da1f-42a0-a271-ede563148afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
